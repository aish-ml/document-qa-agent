# ============================================
# Document Q&A AI Agent - Environment Variables
# ============================================
# Copy this file to .env and fill in your API keys.

# --- LLM Provider (choose one or both) ---
# Options: openai, gemini, ollama
LLM_PROVIDER=gemini

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4o

# Google Gemini Configuration
GEMINI_API_KEY=your-gemini-api-key-here
GEMINI_MODEL=gemini-2.0-flash

# Ollama (local open-source models)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3

# --- Embedding Model ---
EMBEDDING_PROVIDER=gemini
EMBEDDING_MODEL=models/gemini-embedding-001

# --- Vector Store ---
CHROMA_PERSIST_DIR=./chroma_db

# --- Document Settings ---
DOCUMENTS_DIR=./documents
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# --- Enterprise Settings ---
MAX_CONTEXT_CHUNKS=8
RESPONSE_MAX_TOKENS=2048
RATE_LIMIT_RPM=60
ENABLE_CACHE=true

# --- Auto-Ingest Settings ---
# Automatically ingest PDFs in documents/ on startup
AUTO_INGEST_ON_START=true
# Default top-N papers to download+ingest from Arxiv search
ARXIV_AUTO_INGEST_TOPN=5
# Batch size for embedding calls during ingestion
INGEST_BATCH_SIZE=50

# --- Runtime LLM Configuration ---
# LLM provider, model, and API key can be changed at runtime
# from the Streamlit sidebar (Settings > LLM Settings).
# Runtime values are session-only and are NOT written to disk.
